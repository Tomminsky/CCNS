{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Cognitive Neuroscience Practical Assignment 2\n",
    "## Training an MLP on MNIST\n",
    "### Tommy Clausner (s4836219) and Steven Smits (s4237263)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import chainer\n",
    "from chainer.functions.evaluation import accuracy\n",
    "from chainer.functions.loss import softmax_cross_entropy\n",
    "from chainer import link\n",
    "from chainer import reporter\n",
    "from chainer import optimizers\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.datasets import TupleDataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before the sub assignments, we first define all given functions to be used.\n",
    "Since this week n_train, batchsize, epochs etc were NOT specified, we take full freedom in changing these parameters (in the convolutional network). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mnist(n_train=500, n_test=100, n_dim=1, with_label=True, classes = None):\n",
    "    \"\"\"\n",
    "\n",
    "    :param n_train: nr of training examples per class\n",
    "    :param n_test: nr of test examples per class\n",
    "    :param n_dim: 1 or 3 (for convolutional input)\n",
    "    :param with_label: whether or not to also provide labels\n",
    "    :param classes: if not None, then it selects only those classes, e.g. [0, 1]\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    train_data, test_data = chainer.datasets.get_mnist(ndim=n_dim, withlabel=with_label)\n",
    "\n",
    "    if not classes:\n",
    "        classes = np.arange(10)\n",
    "    n_classes = len(classes)\n",
    "\n",
    "    if with_label:\n",
    "\n",
    "        for d in range(2):\n",
    "\n",
    "            if d==0:\n",
    "                data = train_data._datasets[0]\n",
    "                labels = train_data._datasets[1]\n",
    "                n = n_train\n",
    "            else:\n",
    "                data = test_data._datasets[0]\n",
    "                labels = test_data._datasets[1]\n",
    "                n = n_test\n",
    "\n",
    "            for i in range(n_classes):\n",
    "                lidx = np.where(labels == classes[i])[0][:n]\n",
    "                if i==0:\n",
    "                    idx = lidx\n",
    "                else:\n",
    "                    idx = np.hstack([idx,lidx])\n",
    "\n",
    "            L = np.concatenate([i*np.ones(n) for i in np.arange(n_classes)]).astype('int32')\n",
    "\n",
    "            if d==0:\n",
    "                train_data = TupleDataset(data[idx],L)\n",
    "            else:\n",
    "                test_data = TupleDataset(data[idx],L)\n",
    "\n",
    "    else:\n",
    "\n",
    "        tmp1, tmp2 = chainer.datasets.get_mnist(ndim=n_dim,withlabel=True)\n",
    "\n",
    "        for d in range(2):\n",
    "\n",
    "            if d == 0:\n",
    "                data = train_data\n",
    "                labels = tmp1._datasets[1]\n",
    "                n = n_train\n",
    "            else:\n",
    "                data = test_data\n",
    "                labels = tmp2._datasets[1]\n",
    "                n = n_test\n",
    "\n",
    "            for i in range(n_classes):\n",
    "                lidx = np.where(labels == classes[i])[0][:n]\n",
    "                if i == 0:\n",
    "                    idx = lidx\n",
    "                else:\n",
    "                    idx = np.hstack([idx, lidx])\n",
    "\n",
    "            if d == 0:\n",
    "                train_data = data[idx]\n",
    "            else:\n",
    "                test_data = data[idx]\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "class RandomIterator(object):\n",
    "    \"\"\"\n",
    "    Generates random subsets of data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, batch_size=1):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            data (TupleDataset):\n",
    "            batch_size (int):\n",
    "\n",
    "        Returns:\n",
    "            list of batches consisting of (input, output) pairs\n",
    "        \"\"\"\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.n_batches = len(self.data) // batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        self.idx = -1\n",
    "        self._order = np.random.permutation(len(self.data))[:(self.n_batches * self.batch_size)]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def next(self):\n",
    "\n",
    "        self.idx += 1\n",
    "\n",
    "        if self.idx == self.n_batches:\n",
    "            raise StopIteration\n",
    "\n",
    "        i = self.idx * self.batch_size\n",
    "\n",
    "        # handles unlabeled and labeled data\n",
    "        if isinstance(self.data, np.ndarray):\n",
    "            return self.data[self._order[i:(i + self.batch_size)]]\n",
    "        else:\n",
    "            return list(self.data[self._order[i:(i + self.batch_size)]])\n",
    "\n",
    "class Classifier(link.Chain):\n",
    "\n",
    "    \"\"\"A simple classifier model.\n",
    "    This is an example of chain that wraps another chain. It computes the\n",
    "    loss and accuracy based on a given input/label pair.\n",
    "    Args:\n",
    "        predictor (~chainer.Link): Predictor network.\n",
    "        lossfun (function): Loss function.\n",
    "        accfun (function): Function that computes accuracy.\n",
    "    Attributes:\n",
    "        predictor (~chainer.Link): Predictor network.\n",
    "        lossfun (function): Loss function.\n",
    "        accfun (function): Function that computes accuracy.\n",
    "        y (~chainer.Variable): Prediction for the last minibatch.\n",
    "        loss (~chainer.Variable): Loss value for the last minibatch.\n",
    "        accuracy (~chainer.Variable): Accuracy for the last minibatch.\n",
    "        compute_accuracy (bool): If ``True``, compute accuracy on the forward\n",
    "            computation. The default value is ``True``.\n",
    "    \"\"\"\n",
    "\n",
    "    compute_accuracy = True\n",
    "\n",
    "    def __init__(self, predictor,\n",
    "                 lossfun=softmax_cross_entropy.softmax_cross_entropy,\n",
    "                 accfun=accuracy.accuracy):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.lossfun = lossfun\n",
    "        self.accfun = accfun\n",
    "        self.y = None\n",
    "        self.loss = None\n",
    "        self.accuracy = None\n",
    "\n",
    "        with self.init_scope():\n",
    "            self.predictor = predictor\n",
    "\n",
    "    def __call__(self, *args):\n",
    "        \"\"\"Computes the loss value for an input and label pair.\n",
    "        It also computes accuracy and stores it to the attribute.\n",
    "        Args:\n",
    "            args (list of ~chainer.Variable): Input minibatch.\n",
    "        The all elements of ``args`` but last one are features and\n",
    "        the last element corresponds to ground truth labels.\n",
    "        It feeds features to the predictor and compare the result\n",
    "        with ground truth labels.\n",
    "        Returns:\n",
    "            ~chainer.Variable: Loss value.\n",
    "        \"\"\"\n",
    "\n",
    "        assert len(args) >= 2\n",
    "        x = args[:-1]\n",
    "        t = args[-1]\n",
    "        self.y = None\n",
    "        self.loss = None\n",
    "        self.accuracy = None\n",
    "        self.y = self.predictor(*x)\n",
    "        self.loss = self.lossfun(self.y, t)\n",
    "        reporter.report({'loss': self.loss}, self)\n",
    "        if self.compute_accuracy:\n",
    "            self.accuracy = self.accfun(self.y, t)\n",
    "            reporter.report({'accuracy': self.accuracy}, self)\n",
    "        return self.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Train a deep neural network for 20 epochs consisting of N fully connected layers and 10 units per layer. Compare the performance on training and validation data using networks consisting of N=1, 2 and 3 layers (note: a two-layer model would be equivalent to an MLP). Visualize and interpret the results. Report your conclusions.\n",
    "\n",
    "The perceptron (N=1) seems to do best of all three networks. Both the MLP N=2 and N=3 networks perform similar, even after several attempts. Thus, adding more layers in such a simple model doesn't provide a benefit in the current context. \n",
    "\n",
    "![figure1](http://preview.ibb.co/muYTdw/figure_1.png)\n",
    "![figure2](http://preview.ibb.co/bHPqrG/figure_2.png)\n",
    "![figure3](http://preview.ibb.co/b3VVrG/figure_3.png)\n",
    "\n",
    "\n",
    "###### Create a network consisting of a convolutional layer, a max pooling layer and one fully connected layer. For the convolutional layers, use 5 output channels, a kernel size of 5, stride of 1 and padding of 0. Again plot the loss. Report your conclusions. \n",
    "\n",
    "The Convolutional layer learns quicker and converges at a higher accuracy than the perceptron and \"deep\" networks. It's also a lot slower than the other networks, which begs the question if there is any benefit using a convolutional layer for such simple data as the MNIST. \n",
    "\n",
    "![figure4](http://preview.ibb.co/gqmQQb/figure_4.png)\n",
    "\n",
    "###### Read the Chainer documentation. Add additional components to your model (e.g. one of dropout, batch normalization, other activation functions, etc.). Report if your new architecture outperforms the original convnet architecture. Provide a plot and a written explanation of your observed (better/worse) results.\n",
    "\n",
    "Dropout was added to the training algorithm. This augmentation is used to reduce overfitting to the data. It works by preventing complex co-adaptations on the training data.  \n",
    "In reality, adding the dropout results in a slight accuracy increase. \n",
    "\n",
    "Additionally weight decay was added to the training algorithm. This augmentation prevents convergence to local minima\n",
    "\n",
    "![figure5](http://preview.ibb.co/bVjZJw/figure_5.png)\n",
    "![figure6](http://preview.ibb.co/fPOGWG/figure_6.png)\n",
    "![figure7](http://preview.ibb.co/iywEJw/figure_7.png)\n",
    "\n",
    "In the next cell, we provide the code. First, the MLP (no, 1, 2 hidden layers), convolutional networks (.0, .2, .5 dropout rate) and an additional convolutional network (.5 dropout rate + .0005 weight decay) are defined and then the algorithm that performs it is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP(chainer.Chain):\n",
    "    \"\"\"Multilayered Perceptron with 1 hidden layer. It takes n_units as input\n",
    "        , which is the number of hidden layer units. In addition, it takes n_out as input, specifying output unit of\n",
    "        last layer, this is 10 because we're working with 10 possible [True/False] outcomes.\"\"\"\n",
    "\n",
    "    def __init__(self, n_units, n_out,numl):\n",
    "        super(MLP, self).__init__(\n",
    "            # No need for input number, it can infer this.\n",
    "            l1=L.Linear(None, n_units),  # Input to layer 1\n",
    "            l2=L.Linear(None, n_units),  # Input to layer 2\n",
    "            lo=L.Linear(None, n_units),  # Layer out\n",
    "        )\n",
    "        self.numlayers=numl\n",
    "\n",
    "    def __call__(self, x):\n",
    "        numlayers=self.numlayers\n",
    "        if numlayers>2:\n",
    "            x2 = F.relu(self.l1(x))\n",
    "            x3 = F.relu(self.l2(x2))\n",
    "            y = self.lo(x3)\n",
    "        elif numlayers>1:\n",
    "            x2 = F.relu(self.l1(x))\n",
    "            y = self.lo(x2)\n",
    "        else:\n",
    "            y = self.lo(x)\n",
    "        return y\n",
    "\n",
    "class MLPConv(chainer.Chain):\n",
    "    def __init__(self):\n",
    "        super(MLPConv, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Convolution2D(in_channels=1, out_channels=5, ksize=5, stride=1, pad=0)\n",
    "            self.l2 = L.Linear(None, 10)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x2 = F.relu(self.l1(x))\n",
    "        x3 = F.max_pooling_2d(x2, 2, 2)\n",
    "        y = F.softmax(self.l2(x3))\n",
    "        return y\n",
    "\n",
    "def DoMLP():\n",
    "    \"\"\" Algorithm that implements the Multilayered Perceptron using chainer. Takes no input. Trains MLP on MNIST data obtained\n",
    "        from chainer.It calculates the loss and accuracy using the classifier, which it automatically prints per epoch and plots\n",
    "        when the MLP is done. Furthermore a convolutionary network is implemented as well including variouse dropout rates.\"\"\"\n",
    "    # Stable factors specification\n",
    "    batchsize = 32  # Training batchsize, blackboard specified 32\n",
    "    noise=[0.2,0.5]\n",
    "    for num_layers in range(1, 8): # this is not only used to vary the number of hidden layers in the MLP, but also to switch between different conditions for the convolutionary network\n",
    "        if num_layers > 3: # if convolutionary network\n",
    "            epoch = 50 # for the convolutionary network 50 training epochs are used\n",
    "        else:\n",
    "            epoch = 20  # Training epochs, blackboard specified 20\n",
    "        unit = 10  # Hidden layer units, blackboard specified 10\n",
    "        # Our model of the neural network\n",
    "        if num_layers > 3:  # if convolutionary network\n",
    "            model = MLPConv()\n",
    "        else: # if  not convolutionary network\n",
    "            model = MLP(unit, 10, num_layers)\n",
    "\n",
    "\n",
    "\n",
    "        # Classifier that calculates the loss and accuracy of the model\n",
    "        classifier_model = Classifier(model)\n",
    "\n",
    "        # Setup an optimizer\n",
    "        optimizer = optimizers.SGD()  # Using Stochastic Gradient Descent\n",
    "        optimizer.setup(classifier_model)\n",
    "        if num_layers>6:\n",
    "            optimizer.add_hook(chainer.optimizer.WeightDecay(0.0005)) # Adding additional component to our model: Weight Decay\n",
    "\n",
    "        # Load the MNIST dataset\n",
    "        train, test = get_mnist()  # Get mnist data.\n",
    "\n",
    "        n_epoch = epoch\n",
    "        N = len(train)  # training data size\n",
    "        N_test = len(test)  # test data size\n",
    "\n",
    "        accplot = np.zeros((n_epoch, 1), dtype=float)  # Store  test accuracy for plot\n",
    "        lossplot = np.zeros((n_epoch, 1), dtype=float)  # Store test loss for plot\n",
    "\n",
    "        accplot_train = np.zeros((n_epoch, 1), dtype=float) # Store train accuracy for plot\n",
    "        lossplot_train = np.zeros((n_epoch, 1), dtype=float)  # Store train loss for plot\n",
    "\n",
    "        # Training the MLP for epochs\n",
    "        for epoch in range(1, n_epoch + 1):  # start with epoch 1 (instead of 0)\n",
    "            print('epoch ', epoch) # prompting the word 'epoch ' and the coresponding training epoch to the Python Consol\n",
    "\n",
    "            # training the MLP with the last chainer method from guide; no cleargrads()!\n",
    "            perm = np.random.permutation(N)  # Including extra random permutation for resulted in higher accuracy than only RandomIterator.\n",
    "            Currbatchtrain = RandomIterator(train) # randomize order of training samples\n",
    "\n",
    "            sum_accuracy_train = 0  # Creating a staring variable\n",
    "            sum_loss_train = 0\n",
    "            for i in range(0, N, batchsize):\n",
    "                input = chainer.Variable(np.asarray(Currbatchtrain.data[perm[i:i + batchsize]][0]))\n",
    "                if num_layers > 3: # convolutionary network without dropout\n",
    "                    input = np.asarray(Currbatchtrain.data[perm[i:i + batchsize]][0])\n",
    "                    input = chainer.Variable(input.reshape(input.shape[0], 1, 28, 28)) # change input space to fit the convolutionary layer\n",
    "                elif num_layers > 4: # convolutionary network with dropout\n",
    "                    input = np.asarray(Currbatchtrain.data[perm[i:i + batchsize]][0])\n",
    "                    input = chainer.Variable(input.reshape(input.shape[0], 1, 28, 28)) # change input space to fit the convolutionary layer\n",
    "                    input = chainer.functions.dropout(input, ratio=noise[num_layers-5]) # add two different levels of dropout (0.2, 0.5) (Adding additional component to our model)\n",
    "\n",
    "                target = chainer.Variable(np.asarray(Currbatchtrain.data[perm[i:i + batchsize]][1]))\n",
    "\n",
    "                optimizer.update(classifier_model, input, target)  # Update the model using the classifier.\n",
    "\n",
    "                sum_loss_train += float(classifier_model.loss.data) * len(target.data)  # Times length of current batch for relative impact\n",
    "                sum_accuracy_train += float(classifier_model.accuracy.data) * len(target.data)\n",
    "\n",
    "            print('mean loss =', (sum_loss_train / N), ', Accuracy =', (sum_accuracy_train / N))  # To check values during process.\n",
    "\n",
    "            # Testing the model\n",
    "            sum_accuracy = 0  # Creating a staring variable\n",
    "            sum_loss = 0\n",
    "            perm = np.random.permutation(N_test) # permutation for the indices\n",
    "            for i in range(0, N_test, batchsize):\n",
    "                if num_layers > 3: # if convolutionary network\n",
    "                    input = np.asarray(test[perm[i:i + batchsize]][0])\n",
    "                    input = chainer.Variable(input.reshape(input.shape[0], 1, 28, 28))\n",
    "                else: # if not convolutionary network\n",
    "                    input = chainer.Variable(test[perm[i:i + batchsize]][0])  # No random iteration, because it's just a test\n",
    "                target = chainer.Variable(test[perm[i:i + batchsize]][1])\n",
    "\n",
    "                loss = classifier_model(input, target) # calculating loss for the test samples over which we randomized beforehand according to a neuronal network model that was trained beforehand using the training data 'train' using chainer (a Python toolbox used to create neuronal network models in Python - did you seriousely read this?)\n",
    "\n",
    "                sum_loss += float(loss.data) * len(target.data)  # Times length of current batch for relative impact\n",
    "                sum_accuracy += float(classifier_model.accuracy.data) * len(target.data)\n",
    "            print('mean loss =', (sum_loss / N_test), ', Accuracy =', (sum_accuracy / N_test))  # To check values during process.\n",
    "\n",
    "            accplot[epoch - 1] = sum_accuracy / N_test # averaging values\n",
    "            lossplot[epoch - 1] = sum_loss / N_test\n",
    "\n",
    "            accplot_train[epoch - 1] = sum_accuracy_train / N # Correcting my mistake from last week, where this was N_test\n",
    "            lossplot_train[epoch - 1] = sum_loss_train / N\n",
    "\n",
    "        # Plot the accuracy and loss at the end per epoch\n",
    "        fig, ax1 = plt.subplots()\n",
    "        ax2 = ax1.twinx()\n",
    "\n",
    "        ax1.plot(range(1, n_epoch + 1), accplot, 'r:', range(1, n_epoch + 1), accplot_train, 'r-',\n",
    "                 label='Training set accuracy')\n",
    "        ax1.set_xlabel('Training Epoch')\n",
    "        ax1.set_xticks(range(1, n_epoch + 1))\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.tick_params(axis='y', colors='red')\n",
    "        ax2.plot(range(1, n_epoch + 1), lossplot, 'b:', range(1, n_epoch + 1), lossplot_train, 'b-',\n",
    "                 label='Training set loss')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.tick_params(axis='y', colors='blue')\n",
    "        ax2.legend(['Test set loss', 'Training set loss'], loc=2)\n",
    "        ax1.legend(['Test set accuracy', 'Training set accuracy'], loc=3)\n",
    "        # a different title for each model was created\n",
    "        if num_layers > 6:\n",
    "            plt.title('CCNS - Assignment 2: Conv Layer MLP + dropout rate .5 + weight decay (.0005)- accuracy and loss')\n",
    "            ax1.set_xticks(range(10, n_epoch + 1,10))\n",
    "        elif num_layers > 5:\n",
    "            plt.title('CCNS - Assignment 2: Conv Layer MLP + dropout rate .5 - accuracy and loss')\n",
    "            ax1.set_xticks(range(10, n_epoch + 1,10))\n",
    "        elif num_layers > 4:\n",
    "            plt.title('CCNS - Assignment 2: Conv Layer MLP + dropout rate .2 - accuracy and loss')\n",
    "            ax1.set_xticks(range(10, n_epoch + 1,10))\n",
    "        elif num_layers>3:\n",
    "            plt.title('CCNS - Assignment 2: Conv Layer MLP without dropout - accuracy and loss')\n",
    "            ax1.set_xticks(range(10, n_epoch + 1,10))\n",
    "        else:\n",
    "            plt.title('CCNS - Assignment 2: N = '+str(num_layers)+ ' MLP - accuracy and loss')\n",
    "        plt.show() #plotting\n",
    "        # end of mlp function assignment.\n",
    "        # ok now for real\n",
    "\n",
    "DoMLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Explain in which ways convolution is biologically plausible and biologically implausible.\n",
    "\n",
    "The convolutional network may mimic the way the visual stream is organized throughout the cortex. To elucidate, the neural network has several receptive fields which pass on information to the next layer containing of receptive fields. Similarly, the early visual cortex has receptive fields which are sensitive to very specific input (e.g. orientation, shape). This information is then passed on to higher receptive fields integrating this information for another sensitivity (e.g. angle, object). Lastly, information about what a visual input actually is, is represented in the temporal lobe. Here, it has been shown that even single neurons may represent an entire object. Synaptic strength are represented by the weights in the convolutional network. Thus, the convolutional network and visual stream have some structure in common. However, it doesn't seem likely that higher receptive fields (such as in CovNet) all of a sudden are fully connected to any possible representation of object in the brain. It is more likely that one receptive field may only be connected to the target that represents the final object. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
