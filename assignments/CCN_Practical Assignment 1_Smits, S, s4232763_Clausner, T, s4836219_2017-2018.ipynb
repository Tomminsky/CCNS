{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Cognitive Neuroscience Practical Assignment 1\n",
    "## Training an MLP on MNIST\n",
    "### Tommy Clausner (s4836219) and Steven Smits (s4237263)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named chainer",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-fa30a71ec1c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mchainer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mchainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mchainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msoftmax_cross_entropy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named chainer"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import chainer\n",
    "from chainer.functions.evaluation import accuracy\n",
    "from chainer.functions.loss import softmax_cross_entropy\n",
    "from chainer import link\n",
    "from chainer import reporter\n",
    "from chainer import optimizers\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.datasets import TupleDataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we define all the functions given by the teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mnist(n_train=100, n_test=100, n_dim=1, with_label=True, classes = None):\n",
    "    \"\"\"\n",
    "\n",
    "    :param n_train: nr of training examples per class\n",
    "    :param n_test: nr of test examples per class\n",
    "    :param n_dim: 1 or 3 (for convolutional input)\n",
    "    :param with_label: whether or not to also provide labels\n",
    "    :param classes: if not None, then it selects only those classes, e.g. [0, 1]\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    train_data, test_data = chainer.datasets.get_mnist(ndim=n_dim, withlabel=with_label)\n",
    "\n",
    "    if not classes:\n",
    "        classes = np.arange(10)\n",
    "    n_classes = len(classes)\n",
    "\n",
    "    if with_label:\n",
    "\n",
    "        for d in range(2):\n",
    "\n",
    "            if d==0:\n",
    "                data = train_data._datasets[0]\n",
    "                labels = train_data._datasets[1]\n",
    "                n = n_train\n",
    "            else:\n",
    "                data = test_data._datasets[0]\n",
    "                labels = test_data._datasets[1]\n",
    "                n = n_test\n",
    "\n",
    "            for i in range(n_classes):\n",
    "                lidx = np.where(labels == classes[i])[0][:n]\n",
    "                if i==0:\n",
    "                    idx = lidx\n",
    "                else:\n",
    "                    idx = np.hstack([idx,lidx])\n",
    "\n",
    "            L = np.concatenate([i*np.ones(n) for i in np.arange(n_classes)]).astype('int32')\n",
    "\n",
    "            if d==0:\n",
    "                train_data = TupleDataset(data[idx],L)\n",
    "            else:\n",
    "                test_data = TupleDataset(data[idx],L)\n",
    "\n",
    "    else:\n",
    "\n",
    "        tmp1, tmp2 = chainer.datasets.get_mnist(ndim=n_dim,withlabel=True)\n",
    "\n",
    "        for d in range(2):\n",
    "\n",
    "            if d == 0:\n",
    "                data = train_data\n",
    "                labels = tmp1._datasets[1]\n",
    "                n = n_train\n",
    "            else:\n",
    "                data = test_data\n",
    "                labels = tmp2._datasets[1]\n",
    "                n = n_test\n",
    "\n",
    "            for i in range(n_classes):\n",
    "                lidx = np.where(labels == classes[i])[0][:n]\n",
    "                if i == 0:\n",
    "                    idx = lidx\n",
    "                else:\n",
    "                    idx = np.hstack([idx, lidx])\n",
    "\n",
    "            if d == 0:\n",
    "                train_data = data[idx]\n",
    "            else:\n",
    "                test_data = data[idx]\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "class RandomIterator(object):\n",
    "    \"\"\"\n",
    "    Generates random subsets of data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, batch_size=1):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            data (TupleDataset):\n",
    "            batch_size (int):\n",
    "\n",
    "        Returns:\n",
    "            list of batches consisting of (input, output) pairs\n",
    "        \"\"\"\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.n_batches = len(self.data) // batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        self.idx = -1\n",
    "        self._order = np.random.permutation(len(self.data))[:(self.n_batches * self.batch_size)]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def next(self):\n",
    "\n",
    "        self.idx += 1\n",
    "\n",
    "        if self.idx == self.n_batches:\n",
    "            raise StopIteration\n",
    "\n",
    "        i = self.idx * self.batch_size\n",
    "\n",
    "        # handles unlabeled and labeled data\n",
    "        if isinstance(self.data, np.ndarray):\n",
    "            return self.data[self._order[i:(i + self.batch_size)]]\n",
    "        else:\n",
    "            return list(self.data[self._order[i:(i + self.batch_size)]])\n",
    "\n",
    "class Classifier(link.Chain):\n",
    "\n",
    "    \"\"\"A simple classifier model.\n",
    "    This is an example of chain that wraps another chain. It computes the\n",
    "    loss and accuracy based on a given input/label pair.\n",
    "    Args:\n",
    "        predictor (~chainer.Link): Predictor network.\n",
    "        lossfun (function): Loss function.\n",
    "        accfun (function): Function that computes accuracy.\n",
    "    Attributes:\n",
    "        predictor (~chainer.Link): Predictor network.\n",
    "        lossfun (function): Loss function.\n",
    "        accfun (function): Function that computes accuracy.\n",
    "        y (~chainer.Variable): Prediction for the last minibatch.\n",
    "        loss (~chainer.Variable): Loss value for the last minibatch.\n",
    "        accuracy (~chainer.Variable): Accuracy for the last minibatch.\n",
    "        compute_accuracy (bool): If ``True``, compute accuracy on the forward\n",
    "            computation. The default value is ``True``.\n",
    "    \"\"\"\n",
    "\n",
    "    compute_accuracy = True\n",
    "\n",
    "    def __init__(self, predictor,\n",
    "                 lossfun=softmax_cross_entropy.softmax_cross_entropy,\n",
    "                 accfun=accuracy.accuracy):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.lossfun = lossfun\n",
    "        self.accfun = accfun\n",
    "        self.y = None\n",
    "        self.loss = None\n",
    "        self.accuracy = None\n",
    "\n",
    "        with self.init_scope():\n",
    "            self.predictor = predictor\n",
    "\n",
    "    def __call__(self, *args):\n",
    "        \"\"\"Computes the loss value for an input and label pair.\n",
    "        It also computes accuracy and stores it to the attribute.\n",
    "        Args:\n",
    "            args (list of ~chainer.Variable): Input minibatch.\n",
    "        The all elements of ``args`` but last one are features and\n",
    "        the last element corresponds to ground truth labels.\n",
    "        It feeds features to the predictor and compare the result\n",
    "        with ground truth labels.\n",
    "        Returns:\n",
    "            ~chainer.Variable: Loss value.\n",
    "        \"\"\"\n",
    "\n",
    "        assert len(args) >= 2\n",
    "        x = args[:-1]\n",
    "        t = args[-1]\n",
    "        self.y = None\n",
    "        self.loss = None\n",
    "        self.accuracy = None\n",
    "        self.y = self.predictor(*x)\n",
    "        self.loss = self.lossfun(self.y, t)\n",
    "        reporter.report({'loss': self.loss}, self)\n",
    "        if self.compute_accuracy:\n",
    "            self.accuracy = self.accfun(self.y, t)\n",
    "            reporter.report({'accuracy': self.accuracy}, self)\n",
    "        return self.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We define an MLP with one hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP(chainer.Chain):\n",
    "    \"\"\"Multilayered Perceptron with 1 hidden layer. It takes n_units as input\n",
    "        , which is the number of hidden layer units. In addition, it takes n_out as input, specifying output unit of\n",
    "        last layer, this is 10 because we're working with 10 possible [True/False] outcomes.\"\"\"\n",
    "\n",
    "    def __init__(self, n_units, n_out):\n",
    "        super(MLP, self).__init__(\n",
    "            # No need for input number, it can infer this.\n",
    "            l1 = L.Linear(None, n_units), # Input to layer 1\n",
    "            l2 = L.Linear(None, n_units), # Layer out\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        y = self.l2(h1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The main function that implements all the aforementioned functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DoMLP():\n",
    "    \"\"\" Algorithm that implements the Multilayered Perceptron using chainer. Takes no input. Trains MLP on MNIST data obtained\n",
    "        from chainer.It calculates the loss and accuracy using the classifier, which it automatically prints per epoch and plots\n",
    "        when the MLP is done. \"\"\"\n",
    "    # Stable factors specification\n",
    "    batchsize = 32  # Training batchsize, blackboard specified 32\n",
    "    epoch = 20  # Training epochs, blackboard specified 20\n",
    "    unit = 10  # Hidden layer units, blackboard specified 10\n",
    "\n",
    "    # Our model of the neural network\n",
    "    model = MLP(unit, 10)\n",
    "\n",
    "    # Classifier that calculates the loss and accuracy of the model\n",
    "    classifier_model = Classifier(model)\n",
    "\n",
    "    # Setup an optimizer\n",
    "    optimizer = optimizers.SGD()  # Using Stochastic Gradient Descent\n",
    "    optimizer.setup(classifier_model)\n",
    "\n",
    "    # Load the MNIST dataset\n",
    "    train, test = get_mnist()  # Get mnist data.\n",
    "\n",
    "    n_epoch = epoch\n",
    "    N = len(train)  # training data size\n",
    "    N_test = len(test)  # test data size\n",
    "\n",
    "    accplot = np.zeros((n_epoch, 1), dtype=float)  # Store  test accuracy for plot\n",
    "    lossplot = np.zeros((n_epoch, 1), dtype=float)  # Store test loss for plot\n",
    "\n",
    "    accplot_train = np.zeros((n_epoch, 1), dtype=float) # Store train accuracy for plot\n",
    "    lossplot_train = np.zeros((n_epoch, 1), dtype=float)  # Store train loss for plot\n",
    "\n",
    "    # Training the MLP for epochs\n",
    "    for epoch in range(1, n_epoch + 1):  # start with epoch 1 (instead of 0)\n",
    "        print('epoch', epoch)\n",
    "\n",
    "        # training the MLP with the last chainer method from guide; no cleargrads()!\n",
    "        perm = np.random.permutation(\n",
    "            N)  # Including extra random permutation for resulted in higher accuracy than only RandomIterator.\n",
    "        Currbatchtrain = RandomIterator(train)\n",
    "\n",
    "        sum_accuracy_train = 0  # Creating a staring variable\n",
    "        sum_loss_train = 0\n",
    "        for i in range(0, N, batchsize):\n",
    "            input = chainer.Variable(np.asarray(Currbatchtrain.data[perm[i:i + batchsize]][0]))\n",
    "            target = chainer.Variable(np.asarray(Currbatchtrain.data[perm[i:i + batchsize]][1]))\n",
    "\n",
    "            optimizer.update(classifier_model, input, target)  # Update the model using the classifier.\n",
    "\n",
    "            sum_loss_train += float(classifier_model.loss.data) * len(target.data)  # Times length of current batch for relative impact\n",
    "            sum_accuracy_train += float(classifier_model.accuracy.data) * len(target.data)\n",
    "\n",
    "\n",
    "        # Testing the MLP\n",
    "        sum_accuracy = 0  # Creating a staring variable\n",
    "        sum_loss = 0\n",
    "        for i in range(0, N_test, 100):\n",
    "            idx = np.asarray(list(range(i, i + 100)))\n",
    "            input = chainer.Variable(test[idx][0])  # No random iteration, because it's just a test\n",
    "            target = chainer.Variable(test[idx][1])\n",
    "\n",
    "            loss = classifier_model(input, target)\n",
    "            sum_loss += float(loss.data) * len(target.data)  # Times length of current batch for relative impact\n",
    "            sum_accuracy += float(classifier_model.accuracy.data) * len(target.data)\n",
    "\n",
    "        print('mean loss =', (sum_loss / N_test), ', Accuracy =', sum_accuracy / N_test)  # To check values during process.\n",
    "        accplot[epoch - 1] = sum_accuracy / N_test\n",
    "        lossplot[epoch - 1] = sum_loss / N_test\n",
    "\n",
    "        accplot_train[epoch - 1] = sum_accuracy_train / N_test\n",
    "        lossplot_train[epoch - 1] = sum_loss_train / N_test\n",
    "\n",
    "    # Plot the accuracy and loss at the end per epoch\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    ax1.plot(range(1, n_epoch + 1), accplot, 'r:', range(1, n_epoch + 1), accplot_train, 'r-',\n",
    "             label='training set accuracy')\n",
    "    ax1.set_xlabel('Training Epoch')\n",
    "    ax1.set_xticks(range(1, n_epoch + 1))\n",
    "    ax1.set_ylabel('accuracy')\n",
    "    ax1.tick_params(axis='y', colors='red')\n",
    "    ax2.plot(range(1, n_epoch + 1), lossplot, 'b:', range(1, n_epoch + 1), lossplot_train, 'b-',\n",
    "             label='training set loss')\n",
    "    ax2.set_ylabel('loss')\n",
    "    ax2.tick_params(axis='y', colors='blue')\n",
    "    ax2.legend(['test set loss', 'training set loss'], loc=2)\n",
    "    ax1.legend(['test set accuracy', 'training set accuracy'], loc=3)\n",
    "    plt.title('CCN - Assignment 1: MLP accuracy and loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DoMLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure of the results from our end.\n",
    "\n",
    "![Programming in Python](https://image.ibb.co/c1n3zQ/Figure_1.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
